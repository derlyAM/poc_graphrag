# üîÑ Instrucciones para Re-Ingestar Documentos

## ‚ö†Ô∏è Problema Actual

Los chunks en Qdrant fueron creados con la versi√≥n anterior del c√≥digo que **NO guardaba** los campos `capitulo` y `titulo`. Por eso las b√∫squedas filtradas por cap√≠tulo no funcionan.

## ‚úÖ Soluci√≥n: Re-Ingestar los Documentos

Necesitas volver a procesar los PDFs para que los chunks incluyan toda la metadata necesaria.

### Paso 1: Detener Streamlit

Si tienes Streamlit corriendo, det√©nlo (`Ctrl+C`) para liberar la base de datos Qdrant.

### Paso 2: Re-Ingestar Documentos

```bash
# Activar entorno virtual (si no est√° activo)
source venv/bin/activate  # En Mac/Linux
# o
venv\Scripts\activate  # En Windows

# Re-ingestar PDFs (esto recrear√° la colecci√≥n)
python scripts/01_ingest_pdfs.py
```

**IMPORTANTE**: El script debe usar `recreate_collection=True` para borrar la colecci√≥n anterior y crear una nueva.

### Paso 3: Verificar el Script de Ingesti√≥n

Aseg√∫rate de que tu script `scripts/01_ingest_pdfs.py` est√© configurado correctamente.

Si el script no existe o necesita actualizaci√≥n, aqu√≠ est√° la versi√≥n correcta:

```python
"""
Script to ingest PDF documents into Qdrant.
Extracts PDFs, chunks them, generates embeddings, and loads into Qdrant.
"""
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger
from src.ingest.pdf_extractor import extract_all_pdfs
from src.ingest.chunker import chunk_documents
from src.ingest.vectorizer import vectorize_chunks

def main():
    """Main ingestion function."""
    logger.info("=" * 80)
    logger.info("STARTING DOCUMENT INGESTION")
    logger.info("=" * 80)

    # Define paths
    data_dir = Path(__file__).parent.parent / "data"

    if not data_dir.exists():
        logger.error(f"Data directory not found: {data_dir}")
        return

    # Step 1: Extract PDFs
    logger.info("\n[STEP 1] Extracting PDFs")
    documents = extract_all_pdfs(data_dir)

    if not documents:
        logger.error("No documents were extracted!")
        return

    logger.info(f"Extracted {len(documents)} documents")

    # Step 2: Chunk documents
    logger.info("\n[STEP 2] Chunking documents")
    chunks = chunk_documents(documents)

    logger.info(f"Created {len(chunks)} chunks")

    # Step 3: Vectorize and upload
    logger.info("\n[STEP 3] Generating embeddings and uploading to Qdrant")
    vectorizer = vectorize_chunks(
        chunks,
        recreate_collection=True  # IMPORTANTE: Recrear colecci√≥n
    )

    # Summary
    logger.info("\n" + "=" * 80)
    logger.info("INGESTION COMPLETED SUCCESSFULLY")
    logger.info("=" * 80)
    logger.info(f"Documents processed: {len(documents)}")
    logger.info(f"Total chunks: {len(chunks)}")
    logger.info(f"Total cost: ${vectorizer.total_cost:.6f}")

if __name__ == "__main__":
    main()
```

### Paso 4: Verificar que Funcion√≥

Despu√©s de la re-ingesti√≥n, verifica que los chunks tienen los campos correctos:

```bash
python scripts/test_chapter_queries.py
```

O prueba directamente en Streamlit:

```bash
streamlit run app/streamlit_app.py
```

Y prueba una query como:
- "Resume el cap√≠tulo 2 del acuerdo √∫nico"
- "Dame un resumen del t√≠tulo 4"

### Paso 5: Verificar los Logs

Durante la ingesti√≥n, deber√≠as ver:

```
[STEP 1] Extracting PDFs
Legal structure: X t√≠tulos, Y cap√≠tulos, Z art√≠culos, ...

[STEP 2] Chunking documents
Created ABC chunks for legal document

[STEP 3] Generating embeddings and uploading to Qdrant
```

## üîç Troubleshooting

### Error: "Storage folder is already accessed"

**Causa**: Streamlit u otro proceso est√° usando Qdrant.

**Soluci√≥n**:
1. Det√©n todos los procesos que usen Qdrant (Streamlit, scripts, etc.)
2. Vuelve a intentar

### Error: "No documents were extracted"

**Causa**: No hay PDFs en la carpeta `data/`

**Soluci√≥n**:
1. Verifica que tienes PDFs en `data/`
2. Verifica que los archivos terminan en `.pdf`

### Los chunks no tienen cap√≠tulo/t√≠tulo

**Causa**: El PDF no fue detectado como documento "legal"

**Soluci√≥n**:
1. Verifica los logs: debe decir "Document type detected: legal"
2. Si dice "generic" o "technical", el PDF no tiene suficientes patrones legales (T√çTULO, CAP√çTULO, ART√çCULO)
3. Revisa el PDF y aseg√∫rate de que tiene la estructura adecuada

### A√∫n no funciona despu√©s de re-ingestar

**Debugging**:

1. Verifica que la colecci√≥n fue recreada:
```python
from src.retrieval.vector_search import VectorSearch
searcher = VectorSearch()
stats = searcher.get_collection_stats()
print(stats)
```

2. Inspecciona un chunk manualmente para ver su estructura

3. Revisa los logs del pipeline para ver qu√© filtros se est√°n aplicando

## üìä Datos de Ejemplo

Despu√©s de la re-ingesti√≥n correcta, un chunk deber√≠a verse as√≠:

```json
{
  "chunk_id": "uuid-...",
  "documento_id": "acuerdo_unico_...",
  "documento_nombre": "Acuerdo √önico...",
  "articulo": "4.5.1",
  "capitulo": "2",        // ‚Üê ESTO DEBE EXISTIR
  "titulo": "4",          // ‚Üê ESTO DEBE EXISTIR
  "seccion": null,
  "subseccion": null,
  "texto": "...",
  "citacion_corta": "Art. 4.5.1, Acuerdo...",
  ...
}
```

## ‚úÖ Checklist

- [ ] Streamlit detenido
- [ ] Script de ingesti√≥n actualizado con `recreate_collection=True`
- [ ] PDFs est√°n en carpeta `data/`
- [ ] Re-ingesti√≥n completada sin errores
- [ ] Logs muestran "Document type detected: legal"
- [ ] Test de queries funcionando
- [ ] Streamlit muestra resultados para queries de cap√≠tulos

## üéØ Pr√≥ximos Pasos

Una vez que la re-ingesti√≥n est√© completa:

1. Prueba queries estructurales en Streamlit
2. Verifica que se detectan los filtros correctamente en los logs
3. Confirma que se recuperan m√∫ltiples chunks del mismo cap√≠tulo
4. Valida que los res√∫menes son completos y estructurados
