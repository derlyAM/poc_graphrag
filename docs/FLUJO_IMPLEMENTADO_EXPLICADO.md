# üìñ Flujo Implementado: Sistema RAG Explicado Paso a Paso

> **Para personas sin conocimientos t√©cnicos previos**
>
> Este documento explica c√≥mo funciona el sistema RAG (Retrieval-Augmented Generation) implementado,
> desde que haces una pregunta hasta que obtienes la respuesta.

**Versi√≥n del Sistema**: v1.1.1 (2025-10-21)
**√öltima Actualizaci√≥n**: 2025-10-21

---

## üìö √çndice

1. [¬øQu√© es un Sistema RAG?](#qu√©-es-un-sistema-rag)
2. [Arquitectura General](#arquitectura-general)
3. [Fase 1: Preparaci√≥n (Ingesti√≥n de Documentos)](#fase-1-preparaci√≥n-ingesti√≥n-de-documentos)
4. [Fase 2: Consulta (Cuando Haces una Pregunta)](#fase-2-consulta-cuando-haces-una-pregunta)
5. [Componentes T√©cnicos Detallados](#componentes-t√©cnicos-detallados)
6. [Versiones de Software Utilizadas](#versiones-de-software-utilizadas)
7. [Ejemplos Paso a Paso](#ejemplos-paso-a-paso)

---

## ¬øQu√© es un Sistema RAG?

### Analog√≠a Simple

Imagina que tienes una biblioteca enorme con miles de documentos legales. Cuando alguien te hace una pregunta:

1. **Sin RAG**: Tendr√≠as que leer TODOS los documentos completos para encontrar la respuesta ‚Üí ‚ùå Lento e imposible
2. **Con RAG**: El sistema busca inteligentemente SOLO las partes relevantes y luego genera una respuesta precisa ‚Üí ‚úÖ R√°pido y efectivo

### Los 3 Componentes de RAG

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RAG = Retrieval + Augmented + Generation               ‚îÇ
‚îÇ        (B√∫squeda) (Mejorada)   (Generaci√≥n)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  R: Retrieval      ‚Üí Buscar fragmentos relevantes en la base de datos
  A: Augmented      ‚Üí Mejorar la b√∫squeda con contexto adicional
  G: Generation     ‚Üí Generar respuesta natural usando IA
```

---

## Arquitectura General

### Vista de Alto Nivel

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   USUARIO (Streamlit)                        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  "¬øQu√© dice el art√≠culo 4.5.1 sobre ajustes?"              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SISTEMA RAG (src/pipeline.py)                   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  [Paso 0] Query Enhancement      ‚Üê Mejora la pregunta       ‚îÇ
‚îÇ  [Paso 1] Vector Search          ‚Üê Busca chunks relevantes  ‚îÇ
‚îÇ  [Paso 2] Re-ranking             ‚Üê Ordena por relevancia    ‚îÇ
‚îÇ  [Paso 3] Generate Answer        ‚Üê IA genera respuesta      ‚îÇ
‚îÇ  [Paso 4] Validate Citations     ‚Üê Valida referencias       ‚îÇ
‚îÇ  [Paso 5] Enhance Answer         ‚Üê Agrega fuentes           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  RESPUESTA AL USUARIO                        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  "El art√≠culo 4.5.1 establece que los ajustes..."          ‚îÇ
‚îÇ  Fuente: Art. 4.5.1, Acuerdo 03/2021                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Fase 1: Preparaci√≥n (Ingesti√≥n de Documentos)

> **Cu√°ndo ocurre**: UNA SOLA VEZ cuando cargas los PDFs al sistema
>
> **Comando**: `python scripts/01_ingest_pdfs.py`

### Paso 1.1: Extracci√≥n del PDF

**¬øQu√© hace?**
Convierte el PDF en texto preservando su estructura jer√°rquica.

```
PDF Original                     ‚Üí    Texto Estructurado
‚îú‚îÄ T√çTULO 4                           {
‚îÇ  ‚îú‚îÄ CAP√çTULO 5                        "titulos": [{"numero": "4", ...}],
‚îÇ  ‚îÇ  ‚îî‚îÄ Art√≠culo 4.5.1                 "capitulos": [{"numero": "5", ...}],
‚îÇ  ‚îÇ     Texto del art√≠culo...          "articulos": [{"numero": "4.5.1", ...}]
‚îÇ  ‚îÇ                                   }
```

**Herramienta usada**: `pymupdf4llm` (versi√≥n 0.0.27)

**C√≥digo**: `src/ingest/pdf_extractor.py`

### Paso 1.2: Chunking Jer√°rquico

**¬øQu√© es un "chunk"?**
Un fragmento peque√±o del documento (aproximadamente 500 palabras o tokens).

**¬øPor qu√© chunks peque√±os?**
- ‚úÖ M√°s precisos para buscar
- ‚úÖ Caben en la memoria del modelo de IA
- ‚úÖ Se pueden combinar si necesitas m√°s contexto

```
Documento Completo (200 p√°ginas)
        ‚Üì
Divisi√≥n Inteligente
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Chunk 1: T√≠tulo 4 > Cap√≠tulo 5 > Art. 4.5.1 ‚îÇ
‚îÇ Texto: "Los ajustes proceder√°n cuando..."   ‚îÇ
‚îÇ Tokens: 245                                  ‚îÇ
‚îÇ Parent: Cap√≠tulo 5 (UUID)                   ‚îÇ
‚îÇ Children: [] (no tiene hijos)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Chunk 2: T√≠tulo 4 > Cap√≠tulo 5 > Art. 4.5.2 ‚îÇ
‚îÇ Texto: "Las liberaciones de recursos..."    ‚îÇ
‚îÇ Tokens: 312                                  ‚îÇ
‚îÇ Parent: Cap√≠tulo 5 (UUID)                   ‚îÇ
‚îÇ Children: [] (no tiene hijos)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Procesador**: `DocumentHierarchyProcessor` (src/ingest/document_hierarchy_processor.py)

**Configuraci√≥n**:
- Tama√±o m√°ximo por chunk: **500 tokens** (~400 palabras)
- Solapamiento entre chunks: **50 tokens** (para no perder contexto)

### Paso 1.3: Vectorizaci√≥n (Embeddings)

**¬øQu√© es vectorizaci√≥n?**
Convertir texto en n√∫meros que representan su **significado sem√°ntico**.

```
Texto Original:
"Los ajustes a los proyectos de inversi√≥n aprobados proceder√°n..."

        ‚Üì OpenAI text-embedding-3-small

Vector (1536 n√∫meros):
[0.023, -0.145, 0.891, ..., -0.234]
      ‚Üë
Representa el "significado" del texto en espacio matem√°tico
```

**¬øPor qu√© es √∫til?**
Dos textos con significado similar tendr√°n vectores cercanos, aunque usen palabras diferentes.

**Ejemplo**:
```
"¬øQu√© es un OCAD?"     ‚Üí  Vector A: [0.1, 0.8, ...]
"Define OCAD"          ‚Üí  Vector B: [0.12, 0.79, ...]  ‚Üê Similar!
"Receta de pizza"      ‚Üí  Vector C: [-0.5, 0.2, ...]  ‚Üê Muy diferente
```

**Modelo usado**: `text-embedding-3-small` (OpenAI)
- Dimensiones: 1536
- Costo: $0.02 por 1M tokens

### Paso 1.4: Almacenamiento en Qdrant

**¬øQu√© es Qdrant?**
Una base de datos especializada en guardar y buscar vectores r√°pidamente.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Qdrant Database ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                         ‚îÇ
‚îÇ  Collection: normativa_sgr                             ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ Point ID: uuid-abc-123                      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ Vector: [0.023, -0.145, 0.891, ...]         ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ Payload (Metadata):                         ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   - chunk_id: "uuid-abc-123"                ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   - documento_id: "acuerdo_03_2021"         ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   - texto: "Los ajustes proceder√°n..."      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   - nivel_jerarquico: 3 (art√≠culo)          ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   - titulo: "4"                             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   - capitulo: "5"                           ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   - articulo: "4.5.1"                       ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   - parent_id: "uuid-capitulo-5"            ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   - hierarchy_path: "Doc > T4 > C5 > A4.5.1"‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   - citacion_corta: "Art. 4.5.1, Ac. 03..."‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Total chunks almacenados: ~615                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Almacenamiento**: `./storage/qdrant_local/`

---

## Fase 2: Consulta (Cuando Haces una Pregunta)

> **Cu√°ndo ocurre**: CADA VEZ que haces una pregunta en Streamlit
>
> **Tiempo total**: 3-8 segundos

### üìä Vista General del Flujo

```
Tu Pregunta
    ‚Üì
[0] Query Enhancement (0.1s)      "Mejoro tu pregunta"
    ‚Üì
[1] Vector Search (1-2s)          "Busco 20 fragmentos relevantes"
    ‚Üì
[2] Re-ranking (0.5-1s)           "Ordeno por relevancia REAL"
    ‚Üì
[3] Generate Answer (2-4s)        "IA lee y genera respuesta"
    ‚Üì
[4] Validate Citations (0.1s)     "Verifico que las citas sean correctas"
    ‚Üì
[5] Enhance Answer (0.1s)         "Agrego referencias y formato"
    ‚Üì
Respuesta Final
```

---

### [Paso 0] Query Enhancement

**Archivo**: `src/retrieval/query_enhancer.py`

**¬øQu√© hace?**
Mejora tu pregunta detectando filtros autom√°ticamente.

#### Ejemplo 1: Detecci√≥n de Filtros

```
ENTRADA:
"¬øQu√© dice el cap√≠tulo 3 sobre proyectos?"

PROCESAMIENTO:
1. Detecta patr√≥n "cap√≠tulo 3" ‚Üí filtro: capitulo="3"
2. Tipo de query: "structural" (busca secci√≥n espec√≠fica)
3. Estrategia: "exhaustive" (necesita m√°s chunks)

SALIDA:
{
  "original_query": "¬øQu√© dice el cap√≠tulo 3 sobre proyectos?",
  "enhanced_query": "proyectos inversi√≥n financiaci√≥n",
  "filters": {
    "capitulo": "3"
  },
  "query_type": "structural",
  "retrieval_strategy": "exhaustive",
  "top_k_recommendation": 30  # Usa m√°s chunks
}
```

#### Ejemplo 2: Query Sem√°ntica

```
ENTRADA:
"¬øQu√© requisitos necesito para un proyecto de ciencia?"

PROCESAMIENTO:
1. No detecta patrones estructurales
2. Tipo de query: "semantic" (busca por significado)
3. Estrategia: "standard" (b√∫squeda normal)

SALIDA:
{
  "original_query": "¬øQu√© requisitos necesito para un proyecto de ciencia?",
  "enhanced_query": "requisitos proyecto ciencia tecnolog√≠a innovaci√≥n",
  "filters": {},
  "query_type": "semantic",
  "retrieval_strategy": "standard",
  "top_k_recommendation": 20
}
```

#### Patrones Detectados

| Patr√≥n | Ejemplo | Filtro Generado |
|--------|---------|-----------------|
| `cap√≠tulo N` | "cap√≠tulo 5" | `capitulo="5"` |
| `t√≠tulo N` | "t√≠tulo IV" | `titulo="4"` (convierte romano‚Üín√∫mero) |
| `art√≠culo X.Y.Z` | "art√≠culo 4.5.1.2" | `articulo="4.5.1.2"` |
| `secci√≥n N` | "secci√≥n 6" | `seccion="6"` |
| `anexo N` | "anexo 8" | `anexo_numero="8"` |

**Configuraci√≥n**:
```python
# En config.py
top_k_retrieval: 20  # Chunks a recuperar (default)
```

---

### [Paso 1] Vector Search

**Archivo**: `src/retrieval/vector_search.py`

**¬øQu√© hace?**
Busca los chunks m√°s relevantes usando similitud de vectores.

#### Proceso Detallado

```
1. VECTORIZAR PREGUNTA
   "¬øQu√© dice el cap√≠tulo 5?"
        ‚Üì OpenAI Embedding
   Vector Query: [0.15, -0.23, 0.87, ...]

2. BUSCAR EN QDRANT
   Qdrant compara:
   - Vector Query vs TODOS los vectores almacenados
   - Calcula distancia coseno (0 = lejano, 1 = id√©ntico)

   Resultados ordenados por score:
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Chunk 1: Score 0.92 (muy similar)   ‚îÇ
   ‚îÇ Chunk 2: Score 0.89                 ‚îÇ
   ‚îÇ Chunk 3: Score 0.85                 ‚îÇ
   ‚îÇ ...                                  ‚îÇ
   ‚îÇ Chunk 20: Score 0.71                ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

3. APLICAR FILTROS (si existen)
   Si query enhancement detect√≥ capitulo="5":
   - Qdrant filtra: SOLO chunks con capitulo="5"
   - Esto reduce falsos positivos

4. EXPANDIR CONTEXTO (opcional)
   Para cada chunk encontrado, agrega:
   - Parent chunk (cap√≠tulo completo)
   - Adjacent chunks (art√≠culos vecinos)

   Esto da m√°s contexto al LLM.
```

#### Ejemplo Real

```python
# Input
query = "¬øQu√© dice el cap√≠tulo 5 sobre ajustes?"
filters = {"capitulo": "5"}
top_k = 20

# Proceso
1. Vector query ‚Üí [0.15, -0.23, ...]
2. Qdrant search con filtros
3. Expande contexto

# Output (20 chunks)
[
  {
    "chunk_id": "uuid-1",
    "texto": "Los ajustes proceder√°n cuando...",
    "score": 0.92,  # Score vectorial
    "articulo": "4.5.1",
    "capitulo": "5",
    "hierarchy_path": "Acuerdo > T√≠tulo 4 > Cap√≠tulo 5 > Art√≠culo 4.5.1",
    "citacion_corta": "Art. 4.5.1, Acuerdo 03/2021"
  },
  {
    "chunk_id": "uuid-2",
    "texto": "Las variables susceptibles de ajuste son...",
    "score": 0.89,
    "articulo": "4.5.1.2",
    "capitulo": "5",
    ...
  },
  ...
]
```

**Tiempo**: 1-2 segundos
**Modelo**: `text-embedding-3-small` (OpenAI)

---

### [Paso 2] Re-ranking

**Archivo**: `src/retrieval/reranker.py`

**¬øQu√© hace?**
Re-ordena los chunks usando un modelo m√°s sofisticado que lee **pregunta + texto completo**.

#### ¬øPor qu√© Re-ranking?

**B√∫squeda vectorial** (Paso 1):
- ‚úÖ R√°pida (milisegundos)
- ‚ùå Aproximada (solo compara vectores)
- ‚ùå No lee el texto realmente

**Re-ranking** (Paso 2):
- ‚úÖ Precisa (lee pregunta + texto completo)
- ‚úÖ Detecta relevancia REAL
- ‚ùå M√°s lenta (pero tolerable)

#### Proceso Detallado

```
ENTRADA: 20 chunks del paso anterior

PROCESAMIENTO:

1. CREAR PARES (query, chunk_text)
   [
     ("¬øQu√© dice el cap√≠tulo 5?", "Los ajustes proceder√°n cuando..."),
     ("¬øQu√© dice el cap√≠tulo 5?", "Las variables susceptibles..."),
     ...
   ]

2. CROSS-ENCODER PREDICE RELEVANCIA

   Model: ms-marco-MiniLM-L-12-v2

   Para cada par ‚Üí Score de relevancia (0-1)

   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Chunk 1: Vector Score 0.92 ‚Üí Rerank 0.95 ‚úÖ  ‚îÇ
   ‚îÇ Chunk 5: Vector Score 0.81 ‚Üí Rerank 0.94 ‚¨ÜÔ∏è  ‚îÇ Subi√≥!
   ‚îÇ Chunk 2: Vector Score 0.89 ‚Üí Rerank 0.88 ‚¨áÔ∏è  ‚îÇ Baj√≥
   ‚îÇ Chunk 3: Vector Score 0.85 ‚Üí Rerank 0.82     ‚îÇ
   ‚îÇ ...                                           ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

3. RE-ORDENAR POR RERANK_SCORE

4. SELECCIONAR TOP-K (default: 5)
```

#### Ejemplo Real

```python
# Input
query = "¬øQu√© dice el cap√≠tulo 5 sobre ajustes?"
chunks = [20 chunks del vector search]
top_k = 5

# Modelo
model = "cross-encoder/ms-marco-MiniLM-L-12-v2"

# Output (5 mejores chunks)
[
  {
    "chunk_id": "uuid-1",
    "texto": "Los ajustes proceder√°n cuando...",
    "original_score": 0.92,  # Score vectorial (Paso 1)
    "rerank_score": 0.95,    # Score cross-encoder (Paso 2) ‚¨ÜÔ∏è
    ...
  },
  {
    "chunk_id": "uuid-5",
    "texto": "Variables susceptibles de ajuste...",
    "original_score": 0.81,
    "rerank_score": 0.94,  # ‚¨ÜÔ∏è Subi√≥ gracias al re-ranking!
    ...
  },
  ...
]
```

**Tiempo**: 0.5-1 segundo
**Modelo**: `cross-encoder/ms-marco-MiniLM-L-12-v2`
**Framework**: `sentence-transformers` (versi√≥n 5.1.0)

#### Versi√≥n del Modelo de Re-ranking

**Nombre completo**: `cross-encoder/ms-marco-MiniLM-L-12-v2`

**Caracter√≠sticas**:
- **Tipo**: Cross-Encoder (lee query + documento juntos)
- **Tama√±o**: 12 capas, ~33M par√°metros (MiniLM = versi√≥n compacta)
- **Entrenamiento**: MS MARCO dataset (Microsoft)
- **Uso**: CPU (no requiere GPU)
- **Velocidad**: ~50-100 pares/segundo en CPU est√°ndar
- **Precisi√≥n**: +15-20% mejora vs b√∫squeda vectorial pura

**¬øPor qu√© este modelo?**
- ‚úÖ R√°pido en CPU (no necesitas GPU)
- ‚úÖ Buen balance precisi√≥n/velocidad
- ‚úÖ Entrenado espec√≠ficamente para re-ranking
- ‚úÖ Open source (no cuesta dinero por uso)

**Alternativas evaluadas** (NO usadas):
- `ms-marco-MiniLM-L-6-v2`: M√°s r√°pido pero menos preciso
- `ms-marco-TinyBERT-L-6`: A√∫n m√°s r√°pido pero demasiado impreciso
- `ms-marco-electra-base`: M√°s preciso pero m√°s lento

---

### [Paso 3] Generate Answer

**Archivo**: `src/generation/llm_client.py`

**¬øQu√© hace?**
El modelo de lenguaje (GPT-4o-mini) lee los chunks y genera una respuesta natural.

#### Proceso

```
ENTRADA:
- Pregunta: "¬øQu√© dice el cap√≠tulo 5 sobre ajustes?"
- Contexto: 5 chunks mejor rankeados

PROMPT AL LLM:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Eres un experto en normativa del SGR.              ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ CONTEXTO:                                          ‚îÇ
‚îÇ [Chunk 1]                                          ‚îÇ
‚îÇ Art. 4.5.1 (Acuerdo 03/2021)                      ‚îÇ
‚îÇ Los ajustes a los proyectos de inversi√≥n...       ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ [Chunk 2]                                          ‚îÇ
‚îÇ Art. 4.5.1.2 (Acuerdo 03/2021)                    ‚îÇ
‚îÇ Las variables susceptibles de ajuste son...        ‚îÇ
‚îÇ ...                                                 ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ PREGUNTA:                                          ‚îÇ
‚îÇ ¬øQu√© dice el cap√≠tulo 5 sobre ajustes?           ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ INSTRUCCIONES:                                     ‚îÇ
‚îÇ - Responde bas√°ndote SOLO en el contexto          ‚îÇ
‚îÇ - Cita los art√≠culos espec√≠ficos                  ‚îÇ
‚îÇ - S√© preciso y conciso                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

GPT-4o-mini procesa y genera:

RESPUESTA:
"El Cap√≠tulo 5 del Acuerdo 03 de 2021 regula los ajustes
a proyectos de inversi√≥n aprobados.

Seg√∫n el Art√≠culo 4.5.1, los ajustes proceder√°n cuando:
1. Existan modificaciones en el alcance del proyecto
2. Se requiera actualizaci√≥n de precios
3. Cambios en la normativa aplicable

Las variables susceptibles de ajuste, seg√∫n el Art√≠culo
4.5.1.2, incluyen:
- Cronograma de ejecuci√≥n
- Presupuesto y costos
- Productos y metas

Fuentes: Art. 4.5.1 y 4.5.1.2 del Acuerdo 03/2021"
```

**Modelo usado**: `gpt-4o-mini`
- Contexto m√°ximo: 128k tokens
- Temperatura: 0.1 (respuestas m√°s determin√≠sticas)
- Max tokens output: 800

**Tiempo**: 2-4 segundos
**Costo**: ~$0.0002-0.0005 por consulta

---

### [Paso 4] Validate Citations

**Archivo**: `src/generation/citation_manager.py`

**¬øQu√© hace?**
Verifica que las citas mencionadas en la respuesta realmente existen en los chunks.

```
ENTRADA:
- Answer: "Seg√∫n el Art√≠culo 4.5.1..."
- Chunks: [5 chunks usados como contexto]

VALIDACI√ìN:
1. Extrae menciones en la respuesta:
   - "Art√≠culo 4.5.1"
   - "Art√≠culo 4.5.1.2"

2. Busca en chunks:
   ‚úÖ Art√≠culo 4.5.1 ‚Üí Encontrado en Chunk 1
   ‚úÖ Art√≠culo 4.5.1.2 ‚Üí Encontrado en Chunk 2

3. Genera reporte:
   {
     "valid_citations": 2,
     "invalid_citations": 0,
     "missing_citations": 0,
     "accuracy": 100%
   }
```

**Tiempo**: 0.1 segundos

---

### [Paso 5] Enhance Answer

**Archivo**: `src/generation/citation_manager.py`

**¬øQu√© hace?**
Formatea la respuesta final agregando:
- Referencias bibliogr√°ficas completas
- Enlaces a fuentes
- Metadata adicional

```
ENTRADA:
"Seg√∫n el Art√≠culo 4.5.1, los ajustes proceder√°n..."

SALIDA:
"Seg√∫n el Art√≠culo 4.5.1, los ajustes proceder√°n...

üìö FUENTES CONSULTADAS:
1. Art√≠culo 4.5.1 - Acuerdo √önico del SGR (Acuerdo 03/2021)
   Ubicaci√≥n: T√≠tulo 4 > Cap√≠tulo 5 > Art√≠culo 4.5.1

2. Art√≠culo 4.5.1.2 - Variables susceptibles de ajuste
   Ubicaci√≥n: T√≠tulo 4 > Cap√≠tulo 5 > Art√≠culo 4.5.1.2

üìä CONFIANZA: 95%
‚è±Ô∏è TIEMPO DE RESPUESTA: 5.2 segundos"
```

**Tiempo**: 0.1 segundos

---

## Componentes T√©cnicos Detallados

### 1. Query Enhancer

**Prop√≥sito**: Mejora la consulta del usuario

**Algoritmo**:
```python
def enhance_query(query, documento_id=None):
    # 1. Normalizar query (lowercase, quitar tildes)
    normalized = normalize_text(query)

    # 2. Detectar patrones estructurales
    filters = {}
    if "cap√≠tulo" in normalized:
        filters["capitulo"] = extract_number(query)
    if "art√≠culo" in normalized:
        filters["articulo"] = extract_article_number(query)
    # ... m√°s patrones

    # 3. Determinar tipo de query
    if filters:
        query_type = "structural"
        strategy = "exhaustive"  # Necesita m√°s chunks
    else:
        query_type = "semantic"
        strategy = "standard"

    # 4. Expandir query con sin√≥nimos (opcional)
    enhanced_query = expand_with_synonyms(query)

    return {
        "original_query": query,
        "enhanced_query": enhanced_query,
        "filters": filters,
        "query_type": query_type,
        "retrieval_strategy": strategy
    }
```

### 2. Vector Search

**Prop√≥sito**: B√∫squeda sem√°ntica r√°pida

**Algoritmo**:
```python
def search_with_context(query, top_k=20, filters=None):
    # 1. Generar embedding de la query
    query_vector = openai_embed(query)  # [0.15, -0.23, ...]

    # 2. Buscar en Qdrant con filtros
    results = qdrant.search(
        collection="normativa_sgr",
        query_vector=query_vector,
        limit=top_k,
        filter=build_qdrant_filter(filters)  # capitulo="5", etc.
    )

    # 3. Expandir contexto (opcional)
    if expand_context:
        for chunk in results:
            # Agregar chunks adyacentes
            parent = get_parent_chunk(chunk.parent_id)
            adjacent = get_adjacent_chunks(chunk.chunk_id)
            chunk.context = [parent] + adjacent

    return results
```

**Complejidad**:
- B√∫squeda vectorial: O(log N) gracias a HNSW index
- N = n√∫mero total de chunks (~615)

### 3. Re-ranker

**Prop√≥sito**: Re-ordenar por relevancia real

**Algoritmo**:
```python
def rerank(query, chunks, top_k=5):
    # 1. Crear pares (query, texto)
    pairs = [(query, chunk.texto) for chunk in chunks]

    # 2. Predecir scores con cross-encoder
    # El modelo lee QUERY + TEXTO completo
    model = CrossEncoder("ms-marco-MiniLM-L-12-v2")
    scores = model.predict(pairs)

    # 3. Agregar scores a chunks
    for i, chunk in enumerate(chunks):
        chunk.rerank_score = scores[i]

    # 4. Ordenar por rerank_score
    chunks.sort(key=lambda x: x.rerank_score, reverse=True)

    # 5. Retornar top-k
    return chunks[:top_k]
```

**Cross-Encoder vs Bi-Encoder**:

```
BI-ENCODER (Vector Search):
Query ‚Üí Encoder ‚Üí Vector A
Text  ‚Üí Encoder ‚Üí Vector B
            ‚Üì
    Cosine Similarity(A, B)

‚úÖ R√°pido (vectores pre-computados)
‚ùå Menos preciso (no ve interacci√≥n)

CROSS-ENCODER (Re-ranker):
Query + Text ‚Üí Encoder ‚Üí Relevance Score
                  ‚Üë
        Ve la interacci√≥n completa

‚úÖ Muy preciso
‚ùå M√°s lento (debe procesar cada par)
```

### 4. LLM Client

**Prop√≥sito**: Generar respuesta natural

**Prompt Engineering**:
```python
system_prompt = """
Eres un experto en normativa del Sistema General de Regal√≠as (SGR) de Colombia.

REGLAS:
1. Responde SOLO con informaci√≥n del CONTEXTO proporcionado
2. Cita SIEMPRE los art√≠culos espec√≠ficos
3. Si la informaci√≥n no est√° en el contexto, di "No encontr√© informaci√≥n"
4. Usa lenguaje claro y profesional
5. Estructura la respuesta con vi√±etas si hay m√∫ltiples puntos
"""

user_prompt = f"""
CONTEXTO:
{format_chunks_as_context(chunks)}

PREGUNTA:
{query}

RESPUESTA:
"""
```

**Modelo**: GPT-4o-mini
- Input tokens: ~2000-3000 (contexto + prompt)
- Output tokens: ~200-500 (respuesta)
- Costo por query: ~$0.0003

### 5. Citation Manager

**Prop√≥sito**: Validar y mejorar citas

**Algoritmo de Validaci√≥n**:
```python
def validate_answer(answer, chunks):
    # 1. Extraer menciones de art√≠culos en la respuesta
    pattern = r"Art(?:√≠culo|\.)\s+([\d.]+)"
    mentions = re.findall(pattern, answer)

    # 2. Construir √≠ndice de art√≠culos disponibles en chunks
    available = {chunk.articulo for chunk in chunks}

    # 3. Validar cada menci√≥n
    valid = [m for m in mentions if m in available]
    invalid = [m for m in mentions if m not in available]

    return {
        "valid_citations": len(valid),
        "invalid_citations": len(invalid),
        "accuracy": len(valid) / len(mentions) if mentions else 1.0
    }
```

---

## Versiones de Software Utilizadas

### Dependencias Core

| Paquete | Versi√≥n | Prop√≥sito |
|---------|---------|-----------|
| **Python** | 3.11+ | Lenguaje base |
| **openai** | ‚â•1.10.0 | Embeddings y LLM |
| **qdrant-client** | ‚â•1.7.0 | Base de datos vectorial |
| **sentence-transformers** | 5.1.0 | Re-ranking (cross-encoder) |
| **pymupdf4llm** | 0.0.27 | Extracci√≥n de PDFs |
| **streamlit** | ‚â•1.30.0 | Interfaz de usuario |
| **tiktoken** | ‚â•0.5.2 | Conteo de tokens OpenAI |
| **loguru** | ‚â•0.7.2 | Logging |
| **pydantic** | ‚â•2.5.0 | Validaci√≥n de config |

### Modelos de IA

| Modelo | Versi√≥n/ID | Uso | Costo |
|--------|-----------|-----|-------|
| **OpenAI Embeddings** | `text-embedding-3-small` | Vectorizaci√≥n | $0.02 / 1M tokens |
| **OpenAI LLM** | `gpt-4o-mini` | Generaci√≥n | $0.15 input + $0.60 output / 1M tokens |
| **Cross-Encoder** | `ms-marco-MiniLM-L-12-v2` | Re-ranking | Gratis (local) |

### Configuraci√≥n por Defecto

```python
# Retrieval
TOP_K_RETRIEVAL = 20    # Chunks recuperados
TOP_K_RERANK = 5        # Chunks despu√©s del re-ranking
CHUNK_SIZE = 500        # Tokens por chunk
CHUNK_OVERLAP = 50      # Tokens de solapamiento

# LLM
LLM_MODEL = "gpt-4o-mini"
TEMPERATURE = 0.1       # Respuestas m√°s determin√≠sticas
MAX_TOKENS = 800        # M√°ximo tokens en respuesta

# Embeddings
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSIONS = 1536

# Re-ranking
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-12-v2"
```

---

## Ejemplos Paso a Paso

### Ejemplo 1: Query Estructural

**Input**: "Resume el cap√≠tulo 5 del acuerdo √∫nico"

#### Paso 0: Query Enhancement
```
Detectado: capitulo="5"
Tipo: structural
Estrategia: exhaustive
Top-K recomendado: 30 (m√°s chunks para resumen completo)
```

#### Paso 1: Vector Search
```
B√∫squeda con filtros: capitulo="5"
Chunks recuperados: 30
Tiempo: 1.2s

Top 3 scores:
1. Art. 4.5.1 - Ajustes a proyectos (score: 0.89)
2. Art. 4.5.2 - Liberaci√≥n de recursos (score: 0.87)
3. Art. 4.5.3 - Modificaciones (score: 0.85)
```

#### Paso 2: Re-ranking
```
Re-ordenando 30 chunks...
Top-K final: 15 (extendido para resumen)
Tiempo: 0.8s

Nuevo orden:
1. Art. 4.5.1 (rerank: 0.92) ‚úÖ
2. Art. 4.5.2 (rerank: 0.90) ‚úÖ
3. Art. 4.5.3 (rerank: 0.88) ‚úÖ
```

#### Paso 3: Generate Answer
```
Prompt enviado con 15 chunks de contexto
Tokens input: 4500
Tokens output: 350

Respuesta generada:
"El Cap√≠tulo 5 del T√≠tulo 4 regula los ajustes y liberaciones
de recursos en proyectos de inversi√≥n aprobados.

**Principales disposiciones:**

**Ajustes (Art. 4.5.1):**
- Los ajustes proceder√°n cuando existan modificaciones...
- Variables ajustables: cronograma, presupuesto, productos...

**Liberaciones (Art. 4.5.2):**
- Los recursos se liberar√°n cuando...
..."
```

#### Paso 4-5: Validaci√≥n y Mejora
```
Citas validadas: 15/15 ‚úÖ
Accuracy: 100%

Respuesta mejorada con fuentes:
[Respuesta] +

üìö FUENTES:
1. Art. 4.5.1 - Acuerdo 03/2021
2. Art. 4.5.2 - Acuerdo 03/2021
...
```

**Tiempo total**: 6.8 segundos
**Costo**: $0.0004

---

### Ejemplo 2: Query Sem√°ntica

**Input**: "¬øQu√© es un OCAD y cu√°les son sus funciones?"

#### Paso 0: Query Enhancement
```
No se detectan patrones estructurales
Tipo: semantic
Estrategia: standard
Top-K recomendado: 20
```

#### Paso 1: Vector Search
```
B√∫squeda sem√°ntica sin filtros
Chunks recuperados: 20
Tiempo: 1.0s

Top 3 scores:
1. Definici√≥n de OCAD (score: 0.94)
2. Funciones del OCAD (score: 0.91)
3. Composici√≥n del OCAD (score: 0.88)
```

#### Paso 2: Re-ranking
```
Re-ordenando 20 chunks...
Top-K final: 5
Tiempo: 0.5s

Orden mejorado:
1. Definici√≥n (rerank: 0.96) ‚úÖ Subi√≥
2. Funciones (rerank: 0.94) ‚úÖ
3. Composici√≥n (rerank: 0.89) ‚úÖ
```

#### Paso 3: Generate Answer
```
Tokens input: 2200
Tokens output: 280

Respuesta:
"El OCAD (√ìrgano Colegiado de Administraci√≥n y Decisi√≥n)
es la instancia responsable de definir los proyectos de
inversi√≥n financiados con recursos del SGR.

**Funciones principales:**
1. Aprobar proyectos de inversi√≥n
2. Realizar seguimiento a la ejecuci√≥n
3. Viabilizar modificaciones
..."
```

**Tiempo total**: 4.2 segundos
**Costo**: $0.0002

---

### Ejemplo 3: Query con Filtro M√∫ltiple

**Input**: "Art√≠culo 4.5.1.2 del t√≠tulo 4"

#### Paso 0: Query Enhancement
```
Detectado: titulo="4", articulo="4.5.1.2"
Tipo: specific
Estrategia: precise
Top-K recomendado: 5 (b√∫squeda muy espec√≠fica)
```

#### Paso 1: Vector Search
```
B√∫squeda con filtros: titulo="4" AND articulo="4.5.1.2"
Chunks recuperados: 2 (muy espec√≠fico)
Tiempo: 0.8s
```

#### Paso 2: Re-ranking
```
Solo 2 chunks, no necesita reordenamiento complejo
Top-K final: 2
Tiempo: 0.2s
```

#### Paso 3: Generate Answer
```
Tokens input: 1500
Tokens output: 180

Respuesta:
"El Art√≠culo 4.5.1.2 del Acuerdo 03 de 2021 establece las
variables susceptibles de ajuste en proyectos de inversi√≥n:

1. Cronograma de ejecuci√≥n
2. Presupuesto y costos
3. Productos y metas
4. Ejecutor del proyecto
..."
```

**Tiempo total**: 2.8 segundos (muy r√°pido)
**Costo**: $0.0001

---

## üìä M√©tricas de Performance

### Tiempos Promedio por Paso

| Paso | Tiempo | % del Total |
|------|--------|-------------|
| Query Enhancement | 0.1s | 2% |
| Vector Search | 1.2s | 24% |
| Re-ranking | 0.6s | 12% |
| Generate Answer | 2.8s | 56% |
| Validate Citations | 0.1s | 2% |
| Enhance Answer | 0.2s | 4% |
| **TOTAL** | **5.0s** | **100%** |

### Costos Promedio por Query

| Componente | Costo |
|------------|-------|
| Embedding (query) | $0.00001 |
| LLM Generation | $0.0003 |
| Re-ranking | $0 (local) |
| **Total por query** | **~$0.0003** |

**Estimaci√≥n mensual** (1000 queries):
- Costo: ~$0.30
- Tiempo agregado: ~1.4 horas

---

## üéØ Conclusiones

### ¬øPor Qu√© Este Flujo Funciona?

1. **Query Enhancement**: Mejora la precisi√≥n al detectar filtros autom√°ticamente
2. **Vector Search**: Reduce 615 chunks a 20 candidatos relevantes en <2s
3. **Re-ranking**: Mejora precisi√≥n +15-20% al leer texto completo
4. **GPT-4o-mini**: Balance perfecto entre calidad y costo
5. **Citation Validation**: Garantiza respuestas verificables

### Limitaciones Actuales

- ‚ùå No hay cach√© (Redis planeado para futuro)
- ‚ùå No hay b√∫squeda h√≠brida (vector + keyword BM25)
- ‚ùå No hay sistema multi-agente (LangGraph planeado)
- ‚ùå No hay grafo de conocimiento (Neo4j planeado)

### Pr√≥ximas Mejoras

Ver `docs/ANALISIS_MEJORAS_RAG.md` para roadmap completo.

---

## üìñ Referencias

- **C√≥digo fuente**: `/src/pipeline.py`
- **Configuraci√≥n**: `/src/config.py`
- **Arquitectura**: `docs/ARQUITECTURA_TECNICA.md`
- **Mejoras planeadas**: `docs/ANALISIS_MEJORAS_RAG.md`

---

**√öltima actualizaci√≥n**: 2025-10-21
**Versi√≥n**: v1.1.1
**Mantenido por**: Equipo RAG
